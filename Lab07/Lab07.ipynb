{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lcycSHx_bDUC",
        "o9TM1_3jqAVC",
        "-LRQDVLvgNKi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **THỰC HÀNH: CÁC GIẢI THUẬT PHÂN LOẠI CƠ BẢN**"
      ],
      "metadata": {
        "id": "KD2GtpXtaHrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **A. MỤC TIÊU CHUNG**\n",
        "\n",
        "Mục tiêu  của phàn thực hành Mục tiêu của phần thực hành các giải thuật phân loại dữ liệu (Data Classification Algorithms)\n",
        "được thiết kế để giúp sinh viên phát triển kỹ năng áp dụng các giải thuật phân loại vào việc\n",
        "giải quyết các bài toán thực tế trong khoa học dữ liệu. Cụ thể, phần thực hành nhằm đạt được\n",
        "các mục tiêu sau: Hiểu và triển khai các thuật toán phân loại: Cây quyết định và rừng cây\n",
        "(Decision Tree & Random Forest), Support Vector Machine (SVM) và Bayes ngây thơ (Naive\n",
        "Bayes) thông qua ngôn ngữ lập trình như Python và sử dụng các thư viện Scikit-learn, SciPy,\n",
        "...\n",
        "Hướng dẫn thực hiện việc triển khai xây dựng một mô hình học máy cổ điển với các bước:\n",
        "- Tiền xử lý dữ liệu: Hướng dẫn cách chuẩn bị dữ liệu, bao gồm xử lý giá trị thiếu, chuẩn hóa\n",
        "dữ liệu, và mã hóa các biến phân loại để phù hợp với yêu cầu của các thuật toán phân loại.\n",
        "- Đánh giá và tối ưu hóa mô hình: sinh viên sẽ thực hành đánh giá hiệu suất mô hình thông\n",
        "\n",
        "qua các chỉ số như accuracy, precision, recall, F1-score và sử dụng các kỹ thuật như cross-\n",
        "validation, grid search để tối ưu hóa tham số.\n",
        "\n",
        "- Phân tích và diễn giải kết quả: Phát triển khả năng diễn giải kết quả phân loại, nhận diện các\n",
        "vấn đề như quá khớp (overfitting) và đề xuất cải tiến."
      ],
      "metadata": {
        "id": "yurSwl7raOfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **B. KẾT CẤU THỰC HÀNH**\n",
        "Thực hành gồm 3 phần là\n",
        "* [Giải thuật cây quyết định](#1)\n",
        "* [Giải thuật Support Vector Machine](#2)\n",
        "* [Giải thuật Bayes ngây thơ](#3)"
      ],
      "metadata": {
        "id": "GZdRxr4OaOb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **C. NỘI DUNG THỰC HÀNH**"
      ],
      "metadata": {
        "id": "nAkI5nbBaOYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"1\" ><h4> <strong>2.1. GIẢI THUẬT 1: CÂY QUYẾT ĐỊNH VÀ RỪNG CÂY </strong> </h4></a>"
      ],
      "metadata": {
        "id": "1j2kL8HAbDX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.1.1. Ôn tập lý thuyết**"
      ],
      "metadata": {
        "id": "lcycSHx_bDUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Quy trình khai phá dữ liệu CRISP – DM (Cross Industry Standard Process for Data Mining) là gì ? Quy\n",
        "trình khai phá dữ liệu SEMMA (Sample, Explore, Modify, Model, Access) là gì?\n",
        "\n",
        "Trả lời:\n",
        "\n",
        "Quy trình khai phá dữ liệu CRISP-DM là quy trình \"kinh điển\" 6 bước:\n",
        "1. Hiểu bài toán nghiệp vụ\n",
        "  - Xác định mục tiêu kinh doanh\n",
        "  - Chuyển mục tiêu kinh doanh thành mục tiêu khai phá dữ liệu\n",
        "2. Hiểu dữ liệu\n",
        "  - Thụ thập dữ liệu ban đầu\n",
        "  - Mô tả dữ liệu (số lượng mẫu và đặc trưng, kiểu dữ liệu)\n",
        "  - Khám phá dữ liệu\n",
        "  - Kiểm tra chất lượng dữ liệu  \n",
        "3. Chuẩn bị dữ liệu\n",
        "  - Làm sạch dữ liệu\n",
        "  - Biến đổi chuẩn hóa, mã hóa, tạo đặc trưng mới\n",
        "  - Tích hợp dữ liệu\n",
        "  - Chọn tập dữ liệu cuối cùng đưa vào mô hình\n",
        "4. Xây dựng mô hình\n",
        "  - Chọn thuật toán\n",
        "  - Chia tập huấn luyện và kiểm tra\n",
        "  - Huấn luyện, điều chỉnh tham số và so sánh nhiều mô hình\n",
        "5. Đánh giá\n",
        "  - Đánh giá mô hình theo mục tiêu nghiệp vụ\n",
        "  - Kiểm tra có đạt yêu cầu kinh doanh không, có bias, hay quá khớp hay không\n",
        "  - Quyết định có triểm khai hay cân quay lại bước trước\n",
        "6. Triển khai\n",
        "  - Đưa mÔ hình vào hệ thống thực\n",
        "  - Theo dõi, bảo trì, huấn luyện lại khi dữ liệu thay đổi\n",
        "\n",
        "Quy trình khai phá dữ liệu SEMMA là quy trình cảu SAS, tâp trung nhiều vào phần kỹ thuật dữ liệu\n",
        "1. Sample\n",
        "- Lấy mẫu dữ liệu đại diện\n",
        "- Chia huấn luyện đánh giá và tập kiểm tra\n",
        "2. Explore\n",
        "- Khám phá dữ liệu\n",
        "- Phát hiện quan hệ\n",
        "3. Modify\n",
        "- Làm sạch, biến đổi, tạo feature, chọn feature.\n",
        "- Biến đổi phi tuyến, rời rạc hóa, chuẩn hóa…\n",
        "4. Model\n",
        "- Xây dựng mô hình\n",
        "- Uowsc lượng tham số\n",
        "5. Access\n",
        "- Đánh giá mô hình\n",
        "- So sánh mô hình chọn mô hình tốt nhất\n"
      ],
      "metadata": {
        "id": "WdhZ_t2WbDRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Cây quyết định hoạt động như thế nào? Hãy giải thích các thành phần chính (nút gốc, nút lá, nhánh)\n",
        "và cách cây đưa ra dự đoán.\n",
        "\n",
        "Trả lời\n",
        "1. Bắt đầu từ nút gốc với một mẫu mới\n",
        "2. Kiểm tra điều kiện tại nút hiện tại, đi theo nhánh tương ứng (Đúng/Sai hoặc theo khoảng giá trị)\n",
        "3. Lặp lại việc đi qua các nút trong đến khi gặp nút lá\n",
        "4. Kết quả dự đoán là nhãn/giá trị của nút lá đó\n",
        "\n",
        "Các thành phần chính\n",
        "* Nút gốc\n",
        "  - Nút đầu tiên của cây, chứa toàn bộ dữ liệu huấn luyện\n",
        "  - Thuộc tính được chọn tại nút gốc là thuộc tính giúp \"chia tốt nhất\" tập dữ liệu\n",
        "* Nút lá\n",
        "  - Nút cuối cùng của cây, không bị chia tiếp\n",
        "  - Với bài toán phân loại: chứa nhãn dự đoán\n",
        "  - Với bài toán hồi quy: giá trị số thực\n",
        "* Nhánh\n",
        "  - Đường dẫn tương ứng với két quả của điều kiện tại một nút\n"
      ],
      "metadata": {
        "id": "VAhMOJqqbDOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Các tiêu chí phân tách (splitting criteria) như Gini Index, Entropy, hay Information Gain được sử dụng\n",
        "trong cây quyết định là gì? Chúng khác nhau ra sao?\n",
        "\n",
        "1. Entropy: Đo mức độ hỗn loạn/ không chắc chắn của nhãn lại một tập S\n",
        "2. Infomation Gain (IG): Đo mức giảm Entropy khi tách theo thuộc tính A\n",
        "3. Gini Index: Hay dùng trong CART (Classification And Regression Tree)\n",
        "\n",
        "So sánh nhanh\n",
        "* Entropy + Information Gain\n",
        "  - Dựa trên lý thuyết thông tin.\n",
        "  - ính toán hơi nặng hơn vì dùng log.\n",
        "  - Nhạy hơn với sự thay đổi xác suất nhỏ.\n",
        "* Gini\n",
        "  - Tính nhanh hơn (chỉ bình phương).\n",
        "  - Thường cho kết quả tương tự Entropy trong thực tế.\n",
        "  - Scikit-learn mặc định dùng Gini cho phân loại."
      ],
      "metadata": {
        "id": "flcf0NzabDLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Rừng cây (Random Forest) là gì? Nó khác gì so với một cây quyết định đơn lẻ? Tại sao Random Forest thường có hiệu suất tốt hơn cây quyết định trong các bài toán phân loại?\n",
        "\n",
        "Trả lời\n",
        "\n",
        "Rừng cây là mô hình ensemble gồm nhiều cây quyết định (thường là cây phân loại/hồi quy), mỗi cây được huấn luyện trên một mẫu con của dữ liệu và một tập con của thuộc tính nhằm tạo sự ngẫu nhiên\n",
        "Cách hoạt động:\n",
        "1. Với mỗi cây:\n",
        "  - Lấy ngẫu nhiên có lặp từ tập huấn luyện\n",
        "  - Mỗi lần phân tách nút, chỉ xem xét một tập con ngẫu nhiên\n",
        "<small>Cập nhật lần cuối 9:40 PM Ngày 27/11/2025</small>\n",
        "2. Chọn đặc trưng ngẫu nhiên tại mỗi nút\n",
        "  - Xem xét một tập con ngẫu nhiên\n",
        "3. Huấn luyện nhiều cây quyết định\n",
        "  - Tạo N cây tương ứng\n",
        "  - Mỗi cây học trên nhiều mẫu dữ liệu khác nhau\n",
        "4. Tổng hợp dự đoán\n",
        "  - Nếu phân loại thì chọn nhãn có số phiếu bình chọn nhiều nhất\n",
        "  - Hồi quy thì lấy trung bình\n",
        "\n",
        "Sự khác biệt so với một cây đơn lẻ\n",
        "- Tập hợp nhiều cây -> Khó overfitting\n",
        "- Hiệu suất cao hơn khi có 1 cây\n"
      ],
      "metadata": {
        "id": "t9S_9q3mbDIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Những ưu điểm và hạn chế của cây quyết định và Random Forest là gì? Trong trường hợp nào thì cây\n",
        "quyết định có thể hoạt động kém hiệu quả?\n",
        "\n",
        "Trả lời\n",
        "\n",
        "**Cây quyết định**\n",
        "\n",
        "*Ưu điểm*\n",
        "\n",
        "- Dễ hiểu, trực quan\n",
        "- Không cần chuản hóa dữ liệu\n",
        "\n",
        "*Hạn chế*\n",
        "\n",
        "- Dễ overfitting\n",
        "- kém ổn định\n",
        "\n",
        "**Rừng ngẫu nhiên**\n",
        "\n",
        "*Ưu điểm*\n",
        "\n",
        "- Độ chính xác cao\n",
        "- Chống overfitting tốt\n",
        "\n",
        "*Hạn chế*\n",
        "\n",
        "- Khó diễn giải\n",
        "- Tốn tài nguyên\n",
        "\n",
        "Khi nào nên sử dụng cây quyết định hoạt động hiểu quả\n",
        "- Dữ liệu có cấu trúc rõ ràng các quy luật phân chia dễ thấy\n",
        "- Bài toán có tính diễn giải quan trọng\n",
        "- Dữ liệu có cả biên số và biến phân loại\n",
        "- Chứa nhiều điều kiện\n",
        "- Dữ liệu ít nhiễu\n",
        "- Khi kích thước dữ liệu không quá lớn\n"
      ],
      "metadata": {
        "id": "cZ_DL4ILbDGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Viết đoạn code mẫu bằng Python (sử dụng Scikit-learn) để xây dựng một mô hình cây quyết định\n",
        "không? Hãy mô tả các bước thực hiện\n"
      ],
      "metadata": {
        "id": "HtrmHlyLaOMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True) # load dữ liệu Iris data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # Chia tập train test\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=3) # Áp dụng mô hình với chiều sâu của các nút là 3\n",
        "model.fit(X_train, y_train) # Fit\n",
        "\n",
        "print(\"Accuracy:\", model.score(X_test, y_test)) # Xuất độ chính xác cho mô hình\n"
      ],
      "metadata": {
        "id": "nQxQbnv7o4wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Làm thế nào để triển khai một mô hình Random Forest trong Python? Bạn thường thiết lập các tham\n",
        "số nào (ví dụ: n_estimators, max_depth)?"
      ],
      "metadata": {
        "id": "VlXrzd63b2vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,     # số cây\n",
        "    max_depth=8,          # độ sâu tối đa\n",
        "    max_features=\"sqrt\",  # số đặc trưng khi split\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "qVWtP6gqp0cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Làm thế nào để đánh giá tầm quan trọng của các đặc trưng (feature importance) trong Random\n",
        "Forest bằng Python?"
      ],
      "metadata": {
        "id": "LdUybEItb2rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dùng code\n",
        "importances = rf.feature_importances_\n",
        "print(importances)"
      ],
      "metadata": {
        "id": "0o6oJcK_p5W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Điều chỉnh siêu tham số (hyperparameter tuning) cho cây quyết định hoặc Random Forest chưa? Hãy\n",
        "mô tả cách bạn sử dụng GridSearchCV hoặc RandomizedSearchCV"
      ],
      "metadata": {
        "id": "wK4OFmaIb2oe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **GridSearchCV**\n",
        "\n",
        "Hoạt động bằng cách thử tất cả các tham số được đề cập và áp dụng vào mô hình đến khi tim được tham số tối ưu nhất cv chính là sử dụng cross-validation nhằm giảm độ overfitting cho mô hình"
      ],
      "metadata": {
        "id": "o9TM1_3jqAVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {\n",
        "    \"n_estimators\": [100, 200],\n",
        "    \"max_depth\": [5, 10, None]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(RandomForestClassifier(), params, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "print(grid.best_params_)"
      ],
      "metadata": {
        "id": "bjKyCkYWp9oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **RandomizedSearchCV**\n",
        "\n",
        "Hoạt động dựa trên việc thử ngẫu nhiên các cặp tham số được đề cập bằng thuộc tính `n-inter` với tham số nào nhanh hơn thì lấy tham số đó sử dụng khi có quá nhiều tham số, thời gian huấn luyện tốn kém và cần kết quả tốt nhanh mà không cần thử hết mọi trường hợp"
      ],
      "metadata": {
        "id": "xJLR0jL8qNHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.1.3. Bài tập thực hành 1**"
      ],
      "metadata": {
        "id": "2RhBsfnrdDfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng cây quyết định và rừng cây trên dữ liệu Titanic lấy từ\n",
        "https://www.kaggle.com/code/dmilla/introduction-to-decision-trees-titanic-dataset"
      ],
      "metadata": {
        "id": "mwzTLTtNdz-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Nạp thư viện và dữ liệu**"
      ],
      "metadata": {
        "id": "UxByy7mc7q9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, classification_report, confusion_matrix)\n",
        "import graphviz\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Cài đặt style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")"
      ],
      "metadata": {
        "id": "B9PL3uvd8Ivl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[1] NẠP DỮ LIỆU\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    print(\" Đã nạp dữ liệu thành công!\")\n",
        "except FileNotFoundError:\n",
        "    print(\" Không tìm thấy file dữ liệu\")\n",
        "    print(\"Vui lòng đảm bảo có file: train.csv, test.csv\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\nKích thước train: {train_df.shape}\")\n",
        "print(f\"Kích thước test: {test_df.shape}\")\n",
        "print(f\"\\nCác cột: {train_df.columns.tolist()}\")\n",
        "print(f\"\\n5 dòng đầu tiên:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(f\"\\nGiá trị thiếu trong train:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "print(f\"\\nPhân bố nhãn Survived:\")\n",
        "print(train_df['Survived'].value_counts())"
      ],
      "metadata": {
        "id": "Uw8pkG418M8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tiền xử lý dữ liệu**"
      ],
      "metadata": {
        "id": "lrfH7vX-770y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[2] TIỀN XỬ LÝ DỮ LIỆU\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Hàm tiền xử lý dữ liệu\"\"\"\n",
        "    data = df.copy()\n",
        "\n",
        "    # Xử lý Age: điền giá trị trung vị\n",
        "    data['Age'].fillna(data['Age'].median(), inplace=True)\n",
        "\n",
        "    # Xử lý Embarked: điền mode\n",
        "    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "    # Xử lý Fare: điền median\n",
        "    data['Fare'].fillna(data['Fare'].median(), inplace=True)\n",
        "\n",
        "    # Tạo feature HasCabin\n",
        "    data['HasCabin'] = data['Cabin'].notna().astype(int)\n",
        "\n",
        "    # Tạo FamilySize\n",
        "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n",
        "\n",
        "    # Tạo IsAlone\n",
        "    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n",
        "\n",
        "    # Trích xuất Title từ Name\n",
        "    data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "    # Nhóm các Title hiếm\n",
        "    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n",
        "                                            'Don', 'Dr', 'Major', 'Rev', 'Sir',\n",
        "                                            'Jonkheer', 'Dona'], 'Rare')\n",
        "    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n",
        "    data['Title'] = data['Title'].replace('Ms', 'Miss')\n",
        "    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "    # Mã hóa Title\n",
        "    title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n",
        "    data['Title'] = data['Title'].map(title_mapping)\n",
        "    data['Title'].fillna(0, inplace=True)\n",
        "\n",
        "    # Mã hóa Sex\n",
        "    data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "\n",
        "    # Mã hóa Embarked\n",
        "    embarked_mapping = {'S': 0, 'C': 1, 'Q': 2}\n",
        "    data['Embarked'] = data['Embarked'].map(embarked_mapping)\n",
        "\n",
        "    # Tạo bins cho Age\n",
        "    data['AgeBin'] = pd.cut(data['Age'], bins=[0, 12, 18, 35, 60, 100],\n",
        "                            labels=[0, 1, 2, 3, 4])\n",
        "\n",
        "    # Tạo bins cho Fare\n",
        "    data['FareBin'] = pd.qcut(data['Fare'], q=4, labels=[0, 1, 2, 3], duplicates='drop')\n",
        "\n",
        "    return data\n",
        "\n",
        "# Tiền xử lý dữ liệu train và test\n",
        "train_processed = preprocess_data(train_df)\n",
        "test_processed = preprocess_data(test_df)\n",
        "\n",
        "# Chọn features\n",
        "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked',\n",
        "            'FamilySize', 'IsAlone', 'HasCabin', 'Title']\n",
        "\n",
        "X_full = train_processed[features]\n",
        "y_full = train_processed['Survived']\n",
        "\n",
        "print(f\"\\nHoàn thành tiền xử lý!\")\n",
        "print(f\"Features: {features}\")\n",
        "print(f\"Kích thước X: {X_full.shape}\")"
      ],
      "metadata": {
        "id": "lDJUWhkN8niy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Xây dựng mô hình**"
      ],
      "metadata": {
        "id": "N4pHu36b77aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[3] CHIA DỮ LIỆU TRAIN/VALIDATION\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, random_state=42, stratify=y_full\n",
        ")\n",
        "\n",
        "print(f\"Train set: {X_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}\")"
      ],
      "metadata": {
        "id": "hEPX4TpK8wWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[4] XÂY DỰNG CÂY QUYẾT ĐỊNH\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 4.1. Cây đơn giản với max_depth=3\n",
        "print(\"\\n4.1. Cây quyết định cơ bản (max_depth=3)\")\n",
        "dt_basic = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_basic.fit(X_train, y_train)\n",
        "\n",
        "train_acc_basic = dt_basic.score(X_train, y_train)\n",
        "val_acc_basic = dt_basic.score(X_val, y_val)\n",
        "\n",
        "print(f\"Accuracy trên train: {train_acc_basic:.4f}\")\n",
        "print(f\"Accuracy trên validation: {val_acc_basic:.4f}\")\n",
        "\n",
        "# 4.2. Tìm max_depth tối ưu bằng GridSearchCV\n",
        "print(\"\\n4.2. Tìm tham số tối ưu với GridSearchCV\")\n",
        "\n",
        "dt_params = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, 7, 8, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "grid_dt = GridSearchCV(\n",
        "    dt,\n",
        "    param_grid=dt_params,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_dt.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\n✓ Tham số tốt nhất: {grid_dt.best_params_}\")\n",
        "print(f\"✓ Accuracy tốt nhất (CV): {grid_dt.best_score_:.4f}\")\n",
        "\n",
        "# Đánh giá trên validation\n",
        "best_dt = grid_dt.best_estimator_\n",
        "train_acc_dt = best_dt.score(X_train, y_train)\n",
        "val_acc_dt = best_dt.score(X_val, y_val)\n",
        "\n",
        "print(f\"\\nAccuracy trên train: {train_acc_dt:.4f}\")\n",
        "print(f\"Accuracy trên validation: {val_acc_dt:.4f}\")\n",
        "\n",
        "# 4.3. Vẽ biểu đồ so sánh max_depth\n",
        "print(\"\\n4.3. Vẽ biểu đồ ảnh hưởng của max_depth\")\n",
        "\n",
        "depths = range(1, 15)\n",
        "train_scores = []\n",
        "val_scores = []\n",
        "\n",
        "for depth in depths:\n",
        "    dt_temp = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    dt_temp.fit(X_train, y_train)\n",
        "    train_scores.append(dt_temp.score(X_train, y_train))\n",
        "    val_scores.append(dt_temp.score(X_val, y_val))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(depths, train_scores, 'o-', label='Train Accuracy', linewidth=2)\n",
        "plt.plot(depths, val_scores, 's-', label='Validation Accuracy', linewidth=2)\n",
        "plt.xlabel('Max Depth', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.title('Ảnh hưởng của Max Depth đến Decision Tree', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gA9p8yex802K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[5] XÂY DỰNG RỪNG CÂY (RANDOM FOREST)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 5.1. Random Forest cơ bản\n",
        "print(\"\\n5.1. Random Forest cơ bản (n_estimators=100)\")\n",
        "\n",
        "rf_basic = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_basic.fit(X_train, y_train)\n",
        "\n",
        "train_acc_rf_basic = rf_basic.score(X_train, y_train)\n",
        "val_acc_rf_basic = rf_basic.score(X_val, y_val)\n",
        "\n",
        "print(f\"Accuracy trên train: {train_acc_rf_basic:.4f}\")\n",
        "print(f\"Accuracy trên validation: {val_acc_rf_basic:.4f}\")\n",
        "\n",
        "# 5.2. Tìm tham số tối ưu\n",
        "print(\"\\n5.2. Tìm tham số tối ưu cho Random Forest\")\n",
        "\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_depth': [4, 6, 8, 10],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "grid_rf = GridSearchCV(\n",
        "    rf,\n",
        "    param_grid=rf_params,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\n✓ Tham số tốt nhất: {grid_rf.best_params_}\")\n",
        "print(f\"✓ Accuracy tốt nhất (CV): {grid_rf.best_score_:.4f}\")\n",
        "\n",
        "# Đánh giá\n",
        "best_rf = grid_rf.best_estimator_\n",
        "train_acc_rf = best_rf.score(X_train, y_train)\n",
        "val_acc_rf = best_rf.score(X_val, y_val)\n",
        "\n",
        "print(f\"\\nAccuracy trên train: {train_acc_rf:.4f}\")\n",
        "print(f\"Accuracy trên validation: {val_acc_rf:.4f}\")\n",
        "\n",
        "# 5.3. Feature Importance\n",
        "print(\"\\n5.3. Mức độ quan trọng của các features\")\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': best_rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(feature_importance)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
        "plt.xlabel('Importance', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.title('Feature Importance - Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 5.4. Vẽ biểu đồ ảnh hưởng của n_estimators\n",
        "print(\"\\n5.4. Vẽ biểu đồ ảnh hưởng của n_estimators\")\n",
        "\n",
        "n_trees = range(10, 201, 10)\n",
        "train_scores_rf = []\n",
        "val_scores_rf = []\n",
        "\n",
        "for n in n_trees:\n",
        "    rf_temp = RandomForestClassifier(n_estimators=n, max_depth=6, random_state=42, n_jobs=-1)\n",
        "    rf_temp.fit(X_train, y_train)\n",
        "    train_scores_rf.append(rf_temp.score(X_train, y_train))\n",
        "    val_scores_rf.append(rf_temp.score(X_val, y_val))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_trees, train_scores_rf, 'o-', label='Train Accuracy', linewidth=2)\n",
        "plt.plot(n_trees, val_scores_rf, 's-', label='Validation Accuracy', linewidth=2)\n",
        "plt.xlabel('Number of Trees', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.title('Ảnh hưởng của số cây đến Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pyABzUrq87cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[6] SO SÁNH KẾT QUẢ CÁC MÔ HÌNH\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "models_comparison = pd.DataFrame({\n",
        "    'Model': ['Decision Tree (Basic)', 'Decision Tree (Tuned)',\n",
        "              'Random Forest (Basic)', 'Random Forest (Tuned)'],\n",
        "    'Train Accuracy': [train_acc_basic, train_acc_dt, train_acc_rf_basic, train_acc_rf],\n",
        "    'Validation Accuracy': [val_acc_basic, val_acc_dt, val_acc_rf_basic, val_acc_rf]\n",
        "})\n",
        "\n",
        "print(models_comparison.to_string(index=False))\n",
        "\n",
        "# Confusion Matrix cho mô hình tốt nhất\n",
        "print(\"\\n[7] CONFUSION MATRIX - RANDOM FOREST (TUNED)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "y_pred = best_rf.predict(X_val)\n",
        "cm = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.title('Confusion Matrix - Random Forest (Tuned)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_val, y_pred, target_names=['Not Survived', 'Survived']))\n"
      ],
      "metadata": {
        "id": "Y_jb7Q1Y8__E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.1.4. Bài tập thực hành 2**"
      ],
      "metadata": {
        "id": "-LCAN5rQfCLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng cây quyết định và rừng cây trên dữ liệu bệnh tiểu đường. Dữ liệu lấy từ\n",
        "https://www.kaggle.com/code/tumpanjawat/diabetes-eda-random-forest-hp"
      ],
      "metadata": {
        "id": "q9En9fkEfCHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Nạp thư viện và dữ liệu**"
      ],
      "metadata": {
        "id": "tjqwqtoa-UfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")"
      ],
      "metadata": {
        "id": "G6TYTOwx-uvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[1] NẠP VÀ KHÁM PHÁ DỮ LIỆU\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
        "    print(\"Đã nạp dữ liệu thành công!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Không tìm thấy file 'diabetes_prediction_dataset.csv'\")\n",
        "    print(\"Vui lòng tải dataset từ:\")\n",
        "    print(\"https://www.kaggle.com/code/tumpanjawat/diabetes-eda-random-forest-hp\")\n",
        "    exit()\n",
        "\n",
        "print(f\"\\nKích thước dữ liệu: {df.shape}\")\n",
        "print(f\"\\nCác cột: {df.columns.tolist()}\")\n",
        "print(f\"\\nThông tin dataset:\")\n",
        "print(df.info())\n",
        "print(f\"\\n5 dòng đầu tiên:\")\n",
        "print(df.head())\n",
        "\n",
        "print(f\"\\nThống kê mô tả:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(f\"\\nPhân bố nhãn diabetes:\")\n",
        "print(df['diabetes'].value_counts())\n",
        "print(f\"Tỷ lệ: \\n{df['diabetes'].value_counts(normalize=True)}\")\n",
        "\n",
        "# Kiểm tra giá trị bất thường (0 không hợp lý)\n",
        "print(f\"\\nKiểm tra giá trị 0 (bất thường):\")\n",
        "zero_cols = ['hypertension', 'heart_disease', 'blood_glucose_level', 'HbA1c_level', 'bmi']\n",
        "for col in zero_cols:\n",
        "    zero_count = (df[col] == 0).sum()\n",
        "    print(f\"{col}: {zero_count} giá trị 0 ({zero_count/len(df)*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "I_H2iM-Y-yx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tiền xử lý dữ liệu**"
      ],
      "metadata": {
        "id": "b00WQF0_-UXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[2] TIỀN XỬ LÝ DỮ LIỆU\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "data = df.copy()\n",
        "\n",
        "# Fill NaNs for all columns first\n",
        "# This handles the single NaN found in multiple columns from df.info()\n",
        "for col in data.columns:\n",
        "    if data[col].isnull().any():\n",
        "        if data[col].dtype == 'object':\n",
        "            data[col].fillna(data[col].mode()[0], inplace=True)\n",
        "        else:\n",
        "            data[col].fillna(data[col].median(), inplace=True)\n",
        "\n",
        "# Replace values that are effectively 0 (abnormal) with median\n",
        "print(\"\\nThay thế giá trị 0 bằng median (nếu 0 là bất thường)...\")\n",
        "cols_to_replace_0_with_median = ['blood_glucose_level', 'HbA1c_level', 'bmi']\n",
        "\n",
        "for col in cols_to_replace_0_with_median:\n",
        "    if (data[col] == 0).any(): # Check if there are any 0s to replace\n",
        "        median_val = data[data[col] != 0][col].median()\n",
        "        data[col] = data[col].replace(0, median_val)\n",
        "        print(f\"  {col}: thay {(df[col] == 0).sum()} giá trị 0 (từ df gốc) bằng {median_val:.2f}\")\n",
        "\n",
        "# Create new features\n",
        "print(\"\\nTạo features mới...\")\n",
        "data['BMI_Category'] = pd.cut(data['bmi'],\n",
        "                               bins=[0, 18.5, 25, 30, 100],\n",
        "                               labels=[0, 1, 2, 3]).astype(int)\n",
        "data['Age_Group'] = pd.cut(data['age'],\n",
        "                            bins=[0, 30, 40, 50, 100],\n",
        "                            labels=[0, 1, 2, 3]).astype(int)\n",
        "data['Glucose_Level'] = pd.cut(data['blood_glucose_level'],\n",
        "                                bins=[0, 100, 125, 200, data['blood_glucose_level'].max() + 0.1],\n",
        "                                labels=[0, 1, 2, 3]).astype(int)\n",
        "\n",
        "# One-hot encode categorical features\n",
        "print(\"\\nMã hóa các biến phân loại (One-Hot Encoding)...\")\n",
        "categorical_cols = ['gender', 'smoking_history']\n",
        "data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = data.drop('diabetes', axis=1)\n",
        "y = data['diabetes'] # This `y` will still contain the NaN, but it's handled in the next step.\n",
        "\n",
        "feature_cols = X.columns.tolist() # Update feature_cols to reflect dummy variables\n",
        "\n",
        "print(f\"\\nHoàn thành tiền xử lý!\")\n",
        "print(f\"Số lượng features: {len(feature_cols)}\")\n",
        "print(f\"Features: {feature_cols}\")\n",
        "print(f\"Kích thước X: {X.shape}, y: {y.shape}\")\n",
        "print(f\"\\nKiểm tra kiểu dữ liệu của X:\\n{X.dtypes.value_counts()}\")"
      ],
      "metadata": {
        "id": "bl2XTo3D-3Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Xây dựng mô hình**"
      ],
      "metadata": {
        "id": "SYKGB5Bt-USp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[3] CHIA DỮ LIỆU TRAIN/TEST\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Drop rows with NaN values in y before splitting\n",
        "# Ensure X and y align after dropping NaNs\n",
        "# Find indices where y is not NaN\n",
        "valid_indices = y.dropna().index\n",
        "X_cleaned = X.loc[valid_indices]\n",
        "y_cleaned = y.loc[valid_indices]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_cleaned, y_cleaned, test_size=0.2, random_state=42, stratify=y_cleaned\n",
        ")\n",
        "\n",
        "print(f\"Train set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"Tỷ lệ class trong train: \\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Tỷ lệ class trong test: \\n{y_test.value_counts(normalize=True)}\")\n",
        "feature_cols = X_train.columns.tolist()"
      ],
      "metadata": {
        "id": "HjTeefaR-6cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[5] XÂY DỰNG CÂY QUYẾT ĐỊNH\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 5.1. Decision Tree cơ bản\n",
        "print(\"\\n5.1. Decision Tree cơ bản (max_depth=5)\")\n",
        "dt_basic = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "dt_basic.fit(X_train, y_train)\n",
        "\n",
        "train_acc_basic = dt_basic.score(X_train, y_train)\n",
        "test_acc_basic = dt_basic.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy trên train: {train_acc_basic:.4f}\")\n",
        "print(f\"Accuracy trên test: {test_acc_basic:.4f}\")\n",
        "\n",
        "# 5.2. Tìm tham số tối ưu với GridSearchCV\n",
        "print(\"\\n5.2. Tìm tham số tối ưu với GridSearchCV\")\n",
        "\n",
        "dt_params = {\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8, 10, 12],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 4, 8],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "grid_dt = GridSearchCV(\n",
        "    dt,\n",
        "    param_grid=dt_params,\n",
        "    scoring='roc_auc',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "grid_dt.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\n Tham số tốt nhất: {grid_dt.best_params_}\")\n",
        "print(f\" ROC AUC tốt nhất (CV): {grid_dt.best_score_:.4f}\")\n",
        "\n",
        "best_dt = grid_dt.best_estimator_\n",
        "train_acc_dt = best_dt.score(X_train, y_train)\n",
        "test_acc_dt = best_dt.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nAccuracy trên train: {train_acc_dt:.4f}\")\n",
        "print(f\"Accuracy trên test: {test_acc_dt:.4f}\")\n",
        "\n",
        "# 5.3. Vẽ biểu đồ learning curve\n",
        "print(\"\\n5.3. Vẽ biểu đồ ảnh hưởng của max_depth\")\n",
        "\n",
        "cv_results = pd.DataFrame(grid_dt.cv_results_)\n",
        "depth_results = cv_results[cv_results['param_criterion'] == grid_dt.best_params_['criterion']]\n",
        "depth_results = depth_results.groupby('param_max_depth').agg({\n",
        "    'mean_train_score': 'mean',\n",
        "    'mean_test_score': 'mean',\n",
        "    'std_test_score': 'mean'\n",
        "}).reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.errorbar(depth_results['param_max_depth'],\n",
        "             depth_results['mean_train_score'],\n",
        "             label='Mean Train ROC AUC', marker='o', linewidth=2)\n",
        "plt.errorbar(depth_results['param_max_depth'],\n",
        "             depth_results['mean_test_score'],\n",
        "             yerr=depth_results['std_test_score']/np.sqrt(5),\n",
        "             label='Mean Test ROC AUC ± 1 SE', marker='s', linewidth=2)\n",
        "plt.xlabel('Max Depth', fontsize=12)\n",
        "plt.ylabel('ROC AUC', fontsize=12)\n",
        "plt.title('Ảnh hưởng của Max Depth - Decision Tree', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CGPxNjPL-8X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[6] XÂY DỰNG RANDOM FOREST\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# 6.1. Random Forest cơ bản\n",
        "print(\"\\n6.1. Random Forest cơ bản (n_estimators=100)\")\n",
        "\n",
        "rf_basic = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=8,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_basic.fit(X_train, y_train)\n",
        "\n",
        "train_acc_rf_basic = rf_basic.score(X_train, y_train)\n",
        "test_acc_rf_basic = rf_basic.score(X_test, y_test)\n",
        "\n",
        "print(f\"Accuracy trên train: {train_acc_rf_basic:.4f}\")\n",
        "print(f\"Accuracy trên test: {test_acc_rf_basic:.4f}\")\n",
        "\n",
        "# 6.2. Tìm tham số tối ưu\n",
        "print(\"\\n6.2. Tìm tham số tối ưu cho Random Forest\")\n",
        "\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_depth': [5, 6, 7, 8, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "grid_rf = GridSearchCV(\n",
        "    rf,\n",
        "    param_grid=rf_params,\n",
        "    scoring='roc_auc',\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "grid_rf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\n✓ Tham số tốt nhất: {grid_rf.best_params_}\")\n",
        "print(f\"✓ ROC AUC tốt nhất (CV): {grid_rf.best_score_:.4f}\")\n",
        "\n",
        "best_rf = grid_rf.best_estimator_\n",
        "train_acc_rf = best_rf.score(X_train, y_train)\n",
        "test_acc_rf = best_rf.score(X_test, y_test)\n",
        "\n",
        "print(f\"\\nAccuracy trên train: {train_acc_rf:.4f}\")\n",
        "print(f\"Accuracy trên test: {test_acc_rf:.4f}\")\n",
        "\n",
        "# 6.3. Feature Importance\n",
        "print(\"\\n6.3. Phân tích Feature Importance\")\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Importance': best_rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(feature_importance)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
        "plt.xlabel('Importance', fontsize=12)\n",
        "plt.ylabel('Feature', fontsize=12)\n",
        "plt.title('Feature Importance - Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6.4. Vẽ biểu đồ ảnh hưởng của n_estimators\n",
        "print(\"\\n6.4. Vẽ biểu đồ ảnh hưởng của n_estimators\")\n",
        "\n",
        "n_trees = range(10, 201, 10)\n",
        "train_scores_rf = []\n",
        "test_scores_rf = []\n",
        "fit_times = []\n",
        "\n",
        "import time\n",
        "for n in n_trees:\n",
        "    rf_temp = RandomForestClassifier(\n",
        "        n_estimators=n,\n",
        "        max_depth=grid_rf.best_params_['max_depth'],\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    start_time = time.time()\n",
        "    rf_temp.fit(X_train, y_train)\n",
        "    fit_times.append(time.time() - start_time)\n",
        "    train_scores_rf.append(rf_temp.score(X_train, y_train))\n",
        "    test_scores_rf.append(rf_temp.score(X_test, y_test))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy vs n_estimators\n",
        "axes[0].plot(n_trees, train_scores_rf, 'o-', label='Train Accuracy', linewidth=2)\n",
        "axes[0].plot(n_trees, test_scores_rf, 's-', label='Test Accuracy', linewidth=2)\n",
        "axes[0].set_xlabel('Number of Trees', fontsize=12)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[0].set_title('Accuracy vs Number of Trees', fontsize=12, fontweight='bold')\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Fit time vs n_estimators\n",
        "axes[1].plot(n_trees, fit_times, 'o-', color='red', linewidth=2)\n",
        "axes[1].set_xlabel('Number of Trees', fontsize=12)\n",
        "axes[1].set_ylabel('Fit Time (seconds)', fontsize=12)\n",
        "axes[1].set_title('Training Time vs Number of Trees', fontsize=12, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gS3caYDN-_FN",
        "outputId": "aaf729d2-9fc8-4f24-ebc2-6756f5daffaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[6] XÂY DỰNG RANDOM FOREST\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "6.1. Random Forest cơ bản (n_estimators=100)\n",
            "Accuracy trên train: 0.9713\n",
            "Accuracy trên test: 0.9699\n",
            "\n",
            "6.2. Tìm tham số tối ưu cho Random Forest\n",
            "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"2\" ><h4> <strong>2.2. GIẢI THUẬT 2: SUPPORT VECTOR MACHINE (SVM) </strong> </h4></a>"
      ],
      "metadata": {
        "id": "3jIWoy_FgGEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2.1. Ôn tập lý thuyết**"
      ],
      "metadata": {
        "id": "-LRQDVLvgNKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Giải thuật Support Vector Machine hoạt động như thế nào? Hãy giải thích khái niệm về ranh giới phân\n",
        "tách (hyperplane) và lề (margin)\n",
        "\n",
        "Trả lời\n",
        "\n",
        "Ta có nhóm dữ liệu vẽ trên mặt phẳng. Nhiệm vụ của ta chính là thực hiện vẽ một doạn thằng hoặc một mặt phẳng phân cách nhằm phân tách ra thành những nhóm khác nhau\n",
        "\n",
        "Nhiệm vụ của SVM chính là tìm tối đa hóa margin và hạn chết lỗi phân loại\n",
        "\n",
        "* Hyperplane (Siêu phẳng): Là đường thằng hoặc mặt phẳng hoặc siêu phẳng dừng để tách các lớp dữ liệu\n",
        "\n",
        "* Margin (lề): Khoảng cách từ hyperplane đến các điểm gần nhất\n"
      ],
      "metadata": {
        "id": "c5E09svtjw9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Các vector hỗ trợ (support vectors) có vai trò gì trong SVM? Tại sao chúng quan trọng?\n",
        "\n",
        "Trả lời:\n",
        "\n",
        "Các vecto hỗ trợ chính là những điểm:\n",
        " - Gần hyperplane nhất\n",
        " - Nằm ở \"biên\" của mỗi lớp\n",
        " - Quyết định hoàn toàn vị trí của hyperplane\n",
        "\n",
        "Chúng có vai trò như những trụ cọt chống giữ cây - dịch chuyển cá trụ sẽ thay đổi cấu trúc cây, giúp SVM gọn nhẹ về mặt thông tin quan trọng, tập trung vào trường hợp khó phân tích nhất\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gQ-5xsCsjw3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Sự khác biệt giữa SVM với lề cứng (hard margin) và lề mềm (soft margin) là gì? Khi nào nên sử dụng lề\n",
        "mềm?\n",
        "\n",
        "**Hard Margin SVM**\n",
        "\n",
        "Giả sử dữ liệu hoàn toàn tashc được bằng một dường thẳng/mặt phẳng\n",
        "\n",
        "Thuật toán này yêu cầu: Không được phép sai: tát cả các điểm phải nằm cùng một bên của hyperplane, tối đa hóa margin trong điều kiện phân loại hoàn hảo\n",
        "\n",
        "Thích hợp khi: Dữ liệu ít nhiễu, tách tuyến tính tốt\n",
        "\n",
        "Nhược điểm: Chỉ cần vài điểm nhiễu nằm lẫn sang phía bên khác thì thuật toán không thể tìm thấy hyperplane thích hợp\n",
        "\n",
        "**Soft Margin SVM**\n",
        "\n",
        "Cho phép có thể nằm sai phía hyperplane hoặc nằm bên trong vùng margin (rất gân ranh giới)\n",
        "\n",
        "Bài toán lúc này có 2 mục tiêu chính là vừa tối đa hóa margin và giảm số lỗi mức độ vi phạm\n",
        "\n",
        "Sử dụng khi dữ liệu có nhiễu, ngoại lai và tách không hoàn toàn\n"
      ],
      "metadata": {
        "id": "FRTYKNHjjwxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Hàm nhân (kernel) trong SVM là gì? Hãy giải thích các loại kernel phổ biến (linear, polynomial, RBF) và\n",
        "khi nào nên sử dụng chúng\n",
        "\n",
        "Trả lời:\n",
        "\n",
        "Hàm nhân (kernel) thực hiện ánh xạ dữ liêu từ không gian gốc sang một không gian có số chieuf cao hơn nơi mà các lớp có thể tách tuyến được, thay vì tính toán trong không gian vừa tạo thì SVM dử dụng kernel để tính\n",
        "\n",
        "**Linear Kernel**\n",
        "\n",
        "- Giống với SVM tuyến tính và không biến đổi không gian\n",
        "\n",
        "Dùng khi:\n",
        "\n",
        "- Dữ liệu gần tuyến tính\n",
        "- Các đặc trưng nhiều so với số mẫu\n",
        "\n",
        "**Polynomial Kernel**\n",
        "\n",
        "- Mô hình học biên phân tách phi tuyến đa thức\n",
        "\n",
        "Dùng khi:\n",
        "\n",
        "- Quan hệ giữa các đặc trưng là một đa thức\n",
        "- Bài toán không quá nhiều chiều và có tương tác bậc cao giữa các đặc trưng\n",
        "\n",
        "\n",
        "**RBF Kernel**\n",
        "\n",
        "Dùng khi:\n",
        "- Dữ liệu không tách được tuyến tính\n",
        "- Không biết dạng quan hệ giữa các biến\n",
        "- Số chiều không quá lớn\n",
        "- Có hàng dạng biên phân chi phức tạp\n",
        "\n"
      ],
      "metadata": {
        "id": "O9FUYb4QjwrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Tham số C trong SVM có ý nghĩa gì? Nó ảnh hưởng như thế nào đến hiệu suất của mô hình?\n",
        "\n",
        "Trả lời:\n",
        "\n",
        "C chính là tham số phạt tham gia vào quá trình điều chỉnh độ quan trọng của việc phân loại đúng trên tập dữ liệu\n",
        "\n",
        "C càng lớn tức mô hình càng nghiêm khắc\n",
        "\n",
        "- Không cho phép nhiều điềm sai hơn\n",
        "- Mô hình cố gắng phân loại các điểm sao cho luôn đúng\n",
        "- Sẵn sàng bo cong điểm phân chia\n",
        "- Thu nhỏ margin sao cho có thể ôm sát từng điểm\n",
        "\n",
        "Khi nào nên sử dụng C lớn và C nhỏ\n",
        "\n",
        "C lớn:\n",
        "\n",
        "- Khi dữ liệu ít nhiễu, sạch\n",
        "- Mô hình dộ chính xác và training cực cao\n",
        "- Đòi hỏi phân loại nghiêm ngặt\n",
        "\n",
        "C nhỏ:\n",
        "\n",
        "- Dữ liệu có nhiều nhiễu, không sạch\n",
        "- Mô hình tống quát tốt\n",
        "- Tránh overfitting"
      ],
      "metadata": {
        "id": "aTwQmXqEjwlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Viết đoạn code mẫu bằng Python (sử dụng Scikit-learn) để xây dựng một mô hình SVM cho bài toán\n",
        "phân loại không? Hãy mô tả các bước thực hiện"
      ],
      "metadata": {
        "id": "bUECrG4WjwYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import thư viện\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 2. Load dữ liệu (ví dụ: Iris)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data        # features\n",
        "y = iris.target      # labels\n",
        "\n",
        "# 3. Chia train / test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 4. Tạo pipeline: Scaling -> SVM\n",
        "model = Pipeline([\n",
        "    ('scaler', StandardScaler()),          # chuẩn hóa\n",
        "    ('svm', SVC(kernel='rbf', C=1.0, gamma='scale'))  # mô hình SVM\n",
        "])\n",
        "\n",
        "# 5. Huấn luyện\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Dự đoán\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 7. Đánh giá\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "43WAhy3l-pof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Hàm nào trong Scikit-learn để chuẩn hóa dữ liệu (scaling) trước khi áp dụng SVM? Tại sao bước này\n",
        "quan trọng?\n",
        "\n",
        "Trả lời\n",
        "\n",
        "`StandardScaler` chuẩn hóa theo z-score\n",
        "\n",
        "`MinMaxScaler` đưa đặc trưng vào khoảng 0 đén 1\n",
        "\n",
        "\n",
        "Scaling quan trọng bởi vì VM (nhất là với RBF, polynomial, linear) phụ thuộc mạnh vào khoảng cách giữa các điểm dữ liệu:\n",
        "\n",
        "- Nếu một feature có giá trị rất lớn (ví dụ [0, 1000]) còn feature khác chỉ [0, 1], thì:\n",
        "\n",
        "  - Feature lớn sẽ áp đảo khoảng cách.\n",
        "  - Mô hình gần như “bỏ qua” những feature có thang đo nhỏ.\n",
        "\n",
        "Chuẩn hóa giúp:\n",
        "\n",
        "- Các feature có quy mô tương đương, không feature nào “nặng ký” quá mức.\n",
        "- Tối ưu thuật toán (SMO, gradient) hội tụ ổn định hơn và nhanh hơn.\n",
        "- Việc chọn tham số C, gamma trở nên hợp lý và dễ hơn."
      ],
      "metadata": {
        "id": "AgtJvNQ4j6Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<small>Hoàn thành phân mục 2.2.1 lúc 10:05 PM ngày 28/11/2025</smaill>"
      ],
      "metadata": {
        "id": "y8os84r0_0as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2.3. Bài tập thực hành 1**"
      ],
      "metadata": {
        "id": "hw5qTPSMhbF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng mô hình từ giải thuật SVM trên dữ liệu bệnh tiểu đường. Dữ liệu lấy từ\n",
        "https://www.kaggle.com/code/tumpanjawat/diabetes-eda-random-forest-hp"
      ],
      "metadata": {
        "id": "YdKANbLbhjAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Nạp thư viện và dữ liệu**"
      ],
      "metadata": {
        "id": "kvtEWeoL-jh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Download&Load dữ liệu iris từ datasets của scikit-learn\n",
        "iris = datasets.load_iris()\n",
        "# Hiển thị mô ta dữ liệu, chỉ có trong các bộ dữ liệu chuẩn và mở để học\n",
        "tập và nghiên cứu\n",
        "print(iris.DESCR)\n",
        "# Từ tập dữ liệu ban đầu, tách lấy ma trận biểu diễn các đặc trưng và\n",
        "nhãn.\n",
        "data = iris.data\n",
        "target = iris.target\n",
        "# TODO: Chia dữ liệu và nhãn thành 2 tập dữ liệu huấn luyện và dữ liệu\n",
        "kiểm tra theo tỉ lệ 80:20\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target,\n",
        "test_size\n",
        "= 0.2, random_state=101)"
      ],
      "metadata": {
        "id": "egp8M-d89YLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tiền xử lý dữ liệu**"
      ],
      "metadata": {
        "id": "Nv4QvGob-jh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Xây dựng mô hình**"
      ],
      "metadata": {
        "id": "As8aWTdS-jh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "# khởi tạo mô hình phân lớp\n",
        "clf = svm.SVC()\n",
        "# Sử dụng phương thức 'fit' để huấn luyện mô hình với dữ liệu huấn luyện\n",
        "và nhãn huấn luyện\n",
        "# fit (X,Y) với X là tập các đối tượng, Y là tập nhãn tương ứng của đối\n",
        "tượng.\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "snFxvbQl9ab3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tính độ chính xác trên tập huấn luyện và tập kiểm tra\n",
        "train_acc = clf.score(X_train,y_train)\n",
        "val_acc = clf.score(X_test,y_test)\n",
        "print('Training accuracy: {}'.format(train_acc))\n",
        "print('Validation accuracy: {}'.format(val_acc))"
      ],
      "metadata": {
        "id": "EjUGCK2I9aYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_svm, best_val_acc và best_kernel lần lượt là các biến lưu mô hình tốt nhất,\n",
        "# độ chính xác cao nhất trên tập kiểm tra và kernel tốt nhất\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "best_svm = None\n",
        "best_val_acc = -1\n",
        "best_kernel = None\n",
        "# Huấn luyện các mô hình dựa trên dữ liệu huấn luyện và tham số kernel\n",
        "# Tính toán độ chính xác trên tập huấn luyện và tập kiểm tra để tìm được\n",
        "mô hình tốt nhất\n",
        "for i in range(4):\n",
        "clf = svm.SVC(kernel=kernels[i], probability=True)\n",
        "clf.fit(X_train, y_train)\n",
        "tmp_val_acc = clf.score(X_test, y_test)\n",
        "if (tmp_val_acc > best_val_acc):\n",
        "best_val_acc = tmp_val_acc\n",
        "best_svm = clf\n",
        "best_kernel = kernels[i]\n",
        "# Hiển thị mô hình tốt nhất cùng với độ chính xác\n",
        "print(\"Best validation accuracy : {} with kernel: {}\".format(best_val_acc,\n",
        "best_kernel))\n",
        "# Mô hình tốt nhất của bạn nên có độ chính xác xấp xỉ 86,67%"
      ],
      "metadata": {
        "id": "ZkcUzvbm9aVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.2.4. Bài tập thực hành 2**"
      ],
      "metadata": {
        "id": "LRebDsl4hbAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng mô hình từ giải thuật SVM trên dữ liệu các con thú trong rừng. Dữ liệu lấy từ\n",
        "https://www.kaggle.com/code/kareemellithy/animal-condition-predict-svm-knn"
      ],
      "metadata": {
        "id": "EkUfe3Q5hay-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load data\n",
        "file_path = \"data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Encode categorical columns\n",
        "df_encoded = df.copy()\n",
        "\n",
        "le_animal = LabelEncoder()\n",
        "df_encoded['AnimalName'] = le_animal.fit_transform(df['AnimalName'])\n",
        "\n",
        "symptom_cols = ['symptoms1','symptoms2','symptoms3','symptoms4','symptoms5']\n",
        "encoders = {}\n",
        "for col in symptom_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_encoded[col] = le.fit_transform(df[col])\n",
        "    encoders[col] = le\n",
        "\n",
        "le_danger = LabelEncoder()\n",
        "df_encoded['Dangerous'] = le_danger.fit_transform(df['Dangerous'])\n",
        "\n",
        "# Train-test split\n",
        "X = df_encoded.drop('Dangerous', axis=1)\n",
        "y = df_encoded['Dangerous']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM model\n",
        "model = SVC(kernel='rbf')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", acc)\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "ZpZjGdmEC2S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"3\" ><h4> <strong>2.3. GIẢI THUẬT 3: BAYES NGÂY THƠ (NAIVE BAYES) </strong> </h4></a>"
      ],
      "metadata": {
        "id": "fNdudqpRhqAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.3.1. Ôn tập lý thuyết**"
      ],
      "metadata": {
        "id": "BjVXLoZOh0Po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Giải thuật Naive Bayes hoạt động như thế nào? Hãy giải thích định lý Bayes và giả định \"ngây thơ\"\n",
        "trong thuật toán này?\n",
        "\n",
        "Trả lời\n",
        "  - Định lý Bayes cho biết xác suất xảy ra của một lớp C khi biết dữ liệu\n",
        "  - Trong phân loại, ta tìm lớp có xác suất hậu nghiệm lớn nhất\n",
        "  - Nguyên lý Naive Bayes hoạt động với dữ liệu có nhiều thuộc tính rất khó vì cần phân phối chung nhiều chiều"
      ],
      "metadata": {
        "id": "PVqbytNgjiIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Các loại mô hình Naive Bayes (Gaussian, Multinomial, Bernoulli) khác nhau ra sao? Khi nào nên sử dụng\n",
        "từng loại?\n",
        "\n",
        "Trả lời\n",
        "\n",
        "1. Gaussian Naive Bayes\n",
        " - Gỉa định rằng mỗi đặc trưng theo từng lớp tuân theo phân phối chuẩn\n",
        "    - Nguyên lý:\n",
        "     Trung bình, phương sai của tất cả đặc trưng liên tục, khi có mẫu mới, chọn lớp có xác suất lớn nhất  \n",
        " - Dùng cho dữ liệu liên tục\n",
        " - Gỉa định thuộc tính tuân theo phân phối chuẩn trong từng lớp\n",
        "2. Multinomial Naive Bayes\n",
        "- Dùng cho dữ liệu đếm số lần xuất hiện:\n",
        "  - Nguyên lý: Đếm số lần xuất hiện từng đặc trưng trong mỗi lớp, tính xác suất có điều kiện\n",
        "- Dùng cho dữ liệu đếm\n",
        "- Phổ biến trong phân loại văn bản\n",
        "3. Bernoulli Naive Bayes\n",
        "- Dùng khi đặc trưng biểu thị có/không\n",
        "  - Nguyên lý: Với mỗi đặc trưng, tính xác suất xuất hiện, Với mẫu mới, Chọn lớp có xác suất cao nhất\n",
        "- Dùng cho thuộc tính nhị phân (0/1)\n",
        "- Cũng dùng trong text những với dạng"
      ],
      "metadata": {
        "id": "aW53HcXOjhzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Tại sao Naive Bayes được gọi là \"ngây thơ\"? Giả định về tính độc lập của các đặc trưng ảnh hưởng như\n",
        "thế nào đến hiệu suất của mô hình?\n",
        "\n",
        "Trả lời\n",
        "\n",
        "Trong thực tế, các đặc trưng phụ thuộc lẫn nhau những với Naive Bayes giả vở chúng độc lập có điều kiện theo lớp nên chúng được xem là ngây thơ\n",
        "\n",
        "Gỉa định ảnh hưởng đến hiệu suất\n",
        "- Nếu đặc trưng gần độc lập -> Hoạt động rất tốt\n",
        "- Nếu đặc trưng phụ thuộc mạnh\n",
        "  - Xác suất tính ra không \"đúng\" theo nghĩa xác suất thật\n",
        "  - Tuy nhiện nhiều khi xếp hạng lớp vẫn đúng"
      ],
      "metadata": {
        "id": "QXx1MNZGjht1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Ưu điểm và hạn chế của Naive Bayes so với các thuật toán phân loại khác như SVM hoặc Random Forest\n",
        "là gì?\n",
        "\n",
        "Trả lời\n",
        "\n",
        "*Ưu điểm*\n",
        "\n",
        "- Rất nhanh (huấn luyện & dự đoán), phù hợp dữ liệu lớn.\n",
        "- Không cần nhiều dữ liệu để cho kết quả chấp nhận được.\n",
        "- Hoạt động tốt với dữ liệu nhiều chiều (văn bản: hàng chục nghìn feature).\n",
        "- Ít bị overfit hơn nhiều mô hình phức tạp.\n",
        "- Dễ triển khai và giải thích (thống kê tần suất đơn giản).\n",
        "\n",
        "*Hạn chế*\n",
        "\n",
        "- Giả định độc lập thường không đúng, có thể làm giảm độ chính xác.\n",
        "- Với dữ liệu phức tạp, SVM / Random Forest thường chính xác hơn.\n",
        "- Khó nắm bắt quan hệ phi tuyến, tương tác giữa many-features.\n",
        "- Gaussian NB phụ thuộc giả định phân phối chuẩn – nếu sai có thể ảnh hưởng kết quả.\n",
        "\n",
        "*So sánh*\n",
        "\n",
        "**Naive Bayes**\n",
        "\n",
        "- Train rất nhanh\n",
        "- Phù hợp với dữ liệu lơn nhiều chiều\n",
        "- Không có khả năng mô hình hóa phức tạp\n",
        "- Nhạy cảm với tương quan feature\n",
        "- Dễ cài đặt nhất\n",
        "\n",
        "**SVM**\n",
        "\n",
        "- Train chậm\n",
        "- Nếu chọn kernel phù hợp thì hoạt động tốt với dữ liệu lớn, nhiều chiều\n",
        "- Khả năng mô hình hóa cao\n",
        "- Ít nhạy cảm với tương quan giữa đặc trưng\n",
        "- Mức cài đặt trung bình\n",
        "\n",
        "**Random Forest**\n",
        "\n",
        "- Tốc độ train vừa và chậm\n",
        "- Dữ liệu lớn có thẻ chậm và tốn RAM\n",
        "- Khả năng mô hình hóa phức tạp rất cao\n",
        "- Nhảy cảm ít với tương quan giữa các đặc trưng\n",
        "- Mức độ cài đặt trung bình\n"
      ],
      "metadata": {
        "id": "vFi20ggcjhn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Viết đoạn code mẫu bằng Python (sử dụng Scikit-learn) để xây dựng một mô hình Naive Bayes (ví dụ:\n",
        "Gaussian Naive Bayes) không? Hãy mô tả các bước thực hiện"
      ],
      "metadata": {
        "id": "UZHRTVx0jhiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# 1. Load dữ liệu\n",
        "iris = load_iris()\n",
        "X = iris.data        # 4 đặc trưng liên tục\n",
        "y = iris.target      # 3 lớp: 0,1,2\n",
        "\n",
        "# 2. Chia train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Khởi tạo mô hình\n",
        "model = GaussianNB()\n",
        "\n",
        "# 4. Huấn luyện\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Dự đoán\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Đánh giá\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
        "\n",
        "# 7. Thử dự đoán một mẫu mới\n",
        "new_sample = [[5.0, 3.5, 1.5, 0.2]]\n",
        "print(\"Dự đoán lớp cho mẫu mới:\", iris.target_names[model.predict(new_sample)][0])"
      ],
      "metadata": {
        "id": "KSi5pzaz4_h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Làm thế nào để xử lý dữ liệu phân loại (categorical data) trước khi áp dụng Multinomial Naive Bayes\n",
        "trong Python?\n",
        "\n",
        "Trả lời\n",
        "\n",
        "Multinominal NB yêu cầu đặc trưng là số không âm ta có thẻ thực hiện One-Hot Encoding cho phiến phân loại -> Multinomial thực sự mạnh khi đặc trưng là biến **đếm/tần suất**. Nếu trong dataset có nhiều biến liên tục thì Gaussian NB thường được sử dụng nhiều hơn"
      ],
      "metadata": {
        "id": "9r5Fikn1jhR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Naive Bayes thường được sử dụng trong phân loại văn bản (text classification). Bạn có thể giải thích\n",
        "cách triển khai Naive Bayes cho bài toán này không?\n",
        "\n",
        "Trả lời\n",
        "\n",
        "Để thực hiện triển khai trong phân loại văn bản ta thực hiện như sau\n",
        "\n",
        "1. Thu thập dữ liệu văn bản\n",
        "- Tập email, tin tức, review sản phẩm,…\n",
        "2. Tiền xử lý (tùy mức độ):\n",
        "- Chuyển thường (lowercase)\n",
        "- Loại ký tự đặc biệt, số nếu không cần\n",
        "- Bỏ stopwords (the, is, a, …)\n",
        "- Stemming / Lemmatization (tuỳ bài toán)\n",
        "\n",
        "3. Vector hóa văn bản thành số\n",
        "\n",
        "Thường dùng:\n",
        "\n",
        "- `CountVectorizer` (bag-of-words): mỗi cột = 1 từ, giá trị = số lần xuất hiện\n",
        "- hoặc `TfidfVectorizer` (TF-IDF): giống trên nhưng có trọng số.\n",
        "4. Chọn mô hình Naive Bayes\n",
        "- Thường dùng MultinomialNB\n",
        "- Nếu dùng vector nhị phân (từ có/không) → có thể dùng BernoulliNB\n",
        "5. Huấn luyện, đánh giá\n",
        "- Chia train/test\n",
        "- Đánh giá bằng accuracy, precision, recall, F1,…\n",
        "6. Triển khai\n",
        "- Đóng gói vào pipeline để pre-processing + model chạy liền mạch.\n",
        "- Save model (pickle, joblib) để deploy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qdd1Pv3mjq0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tóm tắt**"
      ],
      "metadata": {
        "id": "Aw07MOd78q1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "- **Naive Bayes** = Định lý Bayes + giả định độc lập giữa các feature.\n",
        "- **Gaussian NB**: feature liên tục, phân phối chuẩn.\n",
        "- **Multinomial NB**: feature là đếm/tần suất, cực mạnh trong text.\n",
        "- **Bernoulli NB**: feature nhị phân, “có/không”.\n",
        "2.\n",
        "- Gọi là “ngây thơ” vì giả định **độc lập** thường không đúng, nhưng thực tế hay cho kết quả tốt.\n",
        "3.\n",
        "- So với SVM / Random Forest: **nhanh, đơn giản**, nhưng không mạnh bằng cho dữ liệu phức tạp.\n",
        "4.\n",
        "- Categorical data → cần encode (thường One-Hot) trước khi dùng Multinomial.\n",
        "5.\n",
        "- Text classification → pipeline: *clean text → vector hóa (TF-IDF) → MultinomialNB*."
      ],
      "metadata": {
        "id": "AB9RmXRo73Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.3.3. Bài tập thực hành 1**"
      ],
      "metadata": {
        "id": "_vL9mQinh0Iz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng mô hình Naïve ngây thơ trên tập dữ liệu hành vi của khách hàng lấy tại\n",
        "https://www.kaggle.com/code/arezalo/customer-behaviour-prediction-naive-bayes"
      ],
      "metadata": {
        "id": "BnyMuMPJi3_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Nạp thư viện và dữ liệu**"
      ],
      "metadata": {
        "id": "YguCvtvbBk3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import thư viieenj\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Tải dữ liệu\n",
        "data = pd.read_csv('Customer_Behaviour.csv')\n",
        "\n",
        "# Hiển thị 5 dòng đầu tiên của dữ liệu\n",
        "print(\"Dữ liệu gốc:\")\n",
        "display(data.head())"
      ],
      "metadata": {
        "id": "rUSEUi-7BovR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tiền xử lý dữ liệu**"
      ],
      "metadata": {
        "id": "1H0CS-J9Bpcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loại bỏ các cột không liên quan (User ID)\n",
        "if 'User ID' in data.columns:\n",
        "    data = data.drop(columns=['User ID'], axis=1)\n",
        "\n",
        "# Tiền xử lý: Mã hóa biến phân loại 'Gender' thành số\n",
        "# (Bước này cần thiết vì Naive Bayes trong thư viện sklearn yêu cầu đầu vào là số)\n",
        "le = LabelEncoder()\n",
        "data['Gender'] = le.fit_transform(data['Gender'])\n",
        "\n",
        "# Tách các đặc trưng (X) và nhãn mục tiêu (y)\n",
        "X = data.drop('Purchased', axis=1)\n",
        "y = data['Purchased']\n",
        "\n",
        "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra (80% huấn luyện, 20% kiểm tra)\n",
        "# random_state=42 để kết quả cố định giống nhau mỗi lần chạy\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Kích thước tập huấn luyện:\", X_train.shape)\n",
        "print(\"Kích thước tập kiểm tra:\", X_test.shape)"
      ],
      "metadata": {
        "id": "azPCtQ21CNvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo bộ chuẩn hóa (Scaler)\n",
        "sc = StandardScaler()\n",
        "\n",
        "# Khớp và chuyển đổi dữ liệu huấn luyện (X_train)\n",
        "X_train_scaled = sc.fit_transform(X_train)\n",
        "\n",
        "# Chuyển đổi dữ liệu kiểm tra (X_test)\n",
        "X_test_scaled = sc.transform(X_test)\n",
        "\n",
        "# Hiển thị vài dòng dữ liệu sau khi chuẩn hóa\n",
        "print(\"Dữ liệu Train sau khi chuẩn hóa (5 dòng đầu):\")\n",
        "print(X_train_scaled[:5])"
      ],
      "metadata": {
        "id": "dHIQN4w4CNlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Xây dựng mô hình**"
      ],
      "metadata": {
        "id": "SIwWdTrbBpQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo mô hình\n",
        "# Sử dụng GaussianNB phù hợp cho dữ liệu số liên tục (Age, Salary)\n",
        "classifier = GaussianNB()\n",
        "\n",
        "# Huấn luyện mô hình trên dữ liệu huấn luyện\n",
        "classifier.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "o1rxk7giCSeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dự đoán trên dữ liệu kiểm tra\n",
        "y_pred = classifier.predict(X_test_scaled)\n",
        "\n",
        "# Đánh giá mô hình\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)"
      ],
      "metadata": {
        "id": "e7NZnn4PCZK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dSoRgjmJCavA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2.3.4. Bài tập thực hành 2**"
      ],
      "metadata": {
        "id": "2hT_gs2Wh0CW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng mô hình Naïve ngây thơ trên tập dữ liệu mushroom. Dữ liệu lấy tại\n",
        "https://www.kaggle.com/datasets/uciml/mushroom-classification/data"
      ],
      "metadata": {
        "id": "b8pfeqEji51a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Nạp thư viện và dữ liệu**"
      ],
      "metadata": {
        "id": "J8vdSe5F-qfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "\n",
        "df = pd.read_csv(\"mushrooms.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "U6K8rPI8A7mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Tiền xử lý dữ liệu**"
      ],
      "metadata": {
        "id": "WPPpnrUM-qfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_col = \"class\"\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nTarget counts:\\n\", df[target_col].value_counts())\n",
        "print(\"\\nMissing values:\\n\", df.isnull().sum())"
      ],
      "metadata": {
        "id": "h9kHzJRuBAdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Xây dựng mô hình**"
      ],
      "metadata": {
        "id": "3PLlMHZV-qfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "oe = OrdinalEncoder(dtype=np.int64)\n",
        "X_enc = oe.fit_transform(X)\n",
        "\n",
        "print(\"Encoded shape:\", X_enc.shape)"
      ],
      "metadata": {
        "id": "jRWCXtixBDCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "print(\"Confusion Matrix:\\n\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "cx9xkfvhBEoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, X_enc, y_enc, cv=cv, scoring='accuracy')\n",
        "\n",
        "print(\"Cross-Validation scores:\", scores)\n",
        "print(\"Mean CV:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "j-KCIQ-2BEew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **D. TÓM TẮT THỰC HÀNH**"
      ],
      "metadata": {
        "id": "Hom0Z_L6i9pL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Với 3 phần là phân loại với giải thuật cây quyết định và rừng cây, giải thuật support vector machine (SVM)\n",
        "và giải thuật Bayes ngây thơ được trình bày trong chương đã phần nào giúp sinh viên có thể tiến hình triển\n",
        "phân loại dữ liệu trên một số tập dữ liệu cơ bản.\n",
        "Với việc thực hành giải quyết các bài tập trên lớp và các bài tập ở nhà giúp sinh viên hiểu rõ hơn các khái\n",
        "niệm lý thuyết đã học và áp dụng tốt vào việc giải quyết các bài toán phân loại trong thực tế."
      ],
      "metadata": {
        "id": "wBEtuq7DjGvn"
      }
    }
  ]
}